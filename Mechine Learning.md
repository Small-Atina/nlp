### 一、模型
#### 1.LR（分类模型，判别模型）
+ 用自己的话描述一下LR
+ 为什么LR用sigmoid函数
+ 为什么损失函数使用交叉熵而不用均方差
+ 手推LR参数梯度下降
+ LR 参数求解的方法
+ 怎么防止过拟合
+ 为什么正则可以防止过拟合
+ L1 L2正则的区别
+ 并行化
+ 如果多分类可以使用LR模型吗？
+ 如果数据不可分怎么办？
+ LR的优缺点
+ LR和线性回归的区别
+ LR和最大熵对比
+ LR和bayes比

#### 2.SVM（分类模型，判别模型）
+ 
#### LR和SVM的区别

#### 3.Decision Tree
#### 4.Random Tree
#### 5.GBDT
#### 6.XGboost
#### 7.LightGBM
https://zhuanlan.zhihu.com/p/99069186
#### Random和GBDT区别；GBDT和XGboost区别；LightGBM和XGboost的区别
+ LightGBM 比 XGBoost 快将近 10 倍，内存占用率大约为 XGBoost 的1/6，并且准确率也有提升。
+ LightGBM提出的动机：主要原因就是为了解决 GBDT 在海量数据遇到的问题，让 GBDT 可以更好更快地用于工业实践。
##### LightGBM 优化部分包含以下：
+ 基于 Histogram 的决策树算法
> 1.内存消耗降低；2.寻找每个特征的最优分割点的时间复杂度降低，原来是O(data),现在是O(bins)
+ 带深度限制的 Leaf-wise 的叶子生长策略
+ 直方图做差加速
+ 直接支持类别特征(Categorical Feature)
+ Cache 命中率优化
+ 基于直方图的稀疏特征优化
+ 多线程优化。
##### LightGBM寻找最优分割点的具体方法


#### 8.KNN(K-近邻)（有监督的分类模型）
+ 分类；有监督；有label，没有专门的训练过程
+ K值含义 - 对于一个样本X，要给它分类，首先从数据集中，在X附近找离它最近的K个数据点，将它划分为归属于类别最多的一类
+ 算法流程

#### 9.K-means（无监督的聚类模型）
+ 算法目标：将输入数据划分为k个聚类
+ 聚类算法；非监督模型；无label，有明显的训练过程
+ K值含义- K是事先设定的数字，将数据集分为K个簇，需要依靠人的先验知识
+ 算法流程
#### KNN和K-means区别

#### 10.HMM
#### 11.CRT

#### 12.Bayes
#### 13.EM

https://www.zybuluo.com/rianusr/note/1167141
### 基础知识
#### 数据处理

#### 特征工程
+ 特征表示
+ 特征选择，降维
> 1.缺失值比例（设置一个阈值，如果比例大于这个阈值，可以删除改特征列）
> 2.低方差滤波（一个特征的数值基本一致，也就是方差很低，可以删除）
> 3.高相关滤波（除了label以为ia，看特征之间两两相关性。相关性如果过大，可以删除一列特征）
> 4.随机森林
> 5.前向特征选择
> 6.反向特征消除
> 7.PCA(主成分分析)
> 8.SVD
> 9.t-SNE
+ 特征转换
#### 模型评估
#### 损失函数
####
#### 
